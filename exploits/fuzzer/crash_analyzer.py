"""Crash analysis and exploitability assessment."""

import hashlib
import re
from pathlib import Path
from typing import Dict, List, Optional


class CrashAnalyzer:
    """Analyze crashes for exploitability and patterns."""

    def __init__(self):
        """Initialize crash analyzer."""
        self.crash_patterns = {
            'buffer_overflow': re.compile(rb'A{50,}'),
            'format_string': re.compile(rb'%[sndxp]{3,}'),
            'null_bytes': re.compile(rb'\x00{10,}'),
            'heap_pattern': re.compile(rb'[H]{100,}'),
            'stack_pattern': re.compile(rb'[S]{100,}')
        }

    def analyze_directory(self, crash_dir: str) -> List[Dict]:
        """Analyze all crashes in a directory.

        Args:
            crash_dir: Directory containing crash files

        Returns:
            List of analyzed crash dictionaries
        """
        crash_path = Path(crash_dir)

        if not crash_path.exists():
            return []

        results = []
        for crash_file in crash_path.glob('*'):
            if crash_file.is_file():
                result = self.analyze_crash_file(str(crash_file))
                results.append(result)

        return results

    def analyze_crash_file(self, crash_file: str) -> Dict:
        """Analyze a single crash file.

        Args:
            crash_file: Path to crash file

        Returns:
            Dictionary with crash analysis
        """
        crash_path = Path(crash_file)
        crash_data = crash_path.read_bytes()

        crash_hash = self.calculate_hash(crash_data)
        crash_type = self.detect_crash_type(crash_data)
        exploitability = self.calculate_exploitability(crash_data, crash_type)
        context = self.extract_context(crash_data)

        return {
            'file': str(crash_file),
            'size': len(crash_data),
            'hash': crash_hash,
            'crash_type': crash_type,
            'exploitability': exploitability,
            'context': context,
            'priority': self._calculate_priority(exploitability, crash_type)
        }

    def calculate_hash(self, data: bytes) -> str:
        """Calculate hash of crash data.

        Args:
            data: Crash data

        Returns:
            MD5 hash hex string
        """
        return hashlib.md5(data).hexdigest()

    def detect_crash_type(self, crash_data: bytes) -> str:
        """Detect the type of crash based on data patterns.

        Args:
            crash_data: Crash input data

        Returns:
            Crash type string
        """
        for crash_type, pattern in self.crash_patterns.items():
            if pattern.search(crash_data):
                return crash_type

        # Additional heuristics
        if len(crash_data) > 10000:
            return 'large_input'
        elif len(crash_data) == 0:
            return 'empty_input'
        elif crash_data.count(b'\x00') > len(crash_data) * 0.5:
            return 'null_dereference'

        return 'unknown'

    def calculate_exploitability(
        self,
        crash_data: bytes,
        crash_type: str
    ) -> str:
        """Calculate exploitability score.

        Args:
            crash_data: Crash input data
            crash_type: Detected crash type

        Returns:
            Exploitability rating: high, medium, low, unknown
        """
        signals = {
            'crash_type': crash_type,
            'data_size': len(crash_data),
            'repeating_pattern': self._has_repeating_pattern(crash_data),
            'null_bytes': crash_data.count(b'\x00'),
            'printable_ratio': self._printable_ratio(crash_data)
        }

        score = self.score_exploitability(signals)

        if score >= 7:
            return 'high'
        elif score >= 4:
            return 'medium'
        elif score >= 2:
            return 'low'
        else:
            return 'unknown'

    def score_exploitability(self, signals: Dict) -> int:
        """Score exploitability based on signals.

        Args:
            signals: Dictionary of exploitability signals

        Returns:
            Exploitability score (0-10)
        """
        score = 0
        crash_type = signals.get('crash_type', 'unknown')

        # High value crash types
        if crash_type in ['buffer_overflow', 'format_string']:
            score += 5
        elif crash_type in ['heap_pattern', 'use_after_free']:
            score += 4
        elif crash_type in ['null_dereference']:
            score += 2

        # Check for control signals
        if signals.get('rip_control', False):
            score += 3
        if signals.get('stack_canary_bypass', False):
            score += 2

        # Pattern analysis
        if signals.get('repeating_pattern', False):
            score += 1

        # Size analysis
        data_size = signals.get('data_size', 0)
        if 500 < data_size < 5000:
            score += 1
        elif data_size > 5000:
            score += 2

        return min(score, 10)

    def _has_repeating_pattern(self, data: bytes) -> bool:
        """Check if data has repeating patterns.

        Args:
            data: Input data

        Returns:
            True if repeating pattern detected
        """
        if len(data) < 10:
            return False

        # Check for runs of same byte
        for i in range(256):
            byte_val = bytes([i])
            if byte_val * 20 in data:
                return True

        return False

    def _printable_ratio(self, data: bytes) -> float:
        """Calculate ratio of printable characters.

        Args:
            data: Input data

        Returns:
            Ratio of printable characters (0.0-1.0)
        """
        if not data:
            return 0.0

        printable_count = sum(1 for b in data if 32 <= b <= 126)
        return printable_count / len(data)

    def extract_context(self, crash_data: bytes) -> Dict:
        """Extract context information from crash data.

        Args:
            crash_data: Crash input data

        Returns:
            Dictionary with context information
        """
        return {
            'length': len(crash_data),
            'patterns': self._find_patterns(crash_data),
            'printable_ratio': self._printable_ratio(crash_data),
            'null_byte_count': crash_data.count(b'\x00')
        }

    def _find_patterns(self, data: bytes) -> List[str]:
        """Find interesting patterns in data.

        Args:
            data: Input data

        Returns:
            List of pattern names found
        """
        patterns = []

        for pattern_name, pattern_regex in self.crash_patterns.items():
            if pattern_regex.search(data):
                patterns.append(pattern_name)

        return patterns

    def filter_interesting(self, crashes: List[Dict]) -> List[Dict]:
        """Filter for interesting/exploitable crashes.

        Args:
            crashes: List of analyzed crashes

        Returns:
            List of interesting crashes
        """
        return [
            crash for crash in crashes
            if crash['exploitability'] in ['high', 'medium']
        ]

    def generate_report(self, crashes: List[Dict]) -> Dict:
        """Generate summary report of crashes.

        Args:
            crashes: List of analyzed crashes

        Returns:
            Summary report dictionary
        """
        # Count by exploitability
        by_exploitability = {}
        by_crash_type = {}
        unique_hashes = set()

        for crash in crashes:
            unique_hashes.add(crash['hash'])

            exploitability = crash['exploitability']
            by_exploitability[exploitability] = by_exploitability.get(exploitability, 0) + 1

            crash_type = crash['crash_type']
            by_crash_type[crash_type] = by_crash_type.get(crash_type, 0) + 1

        return {
            'total_crashes': len(crashes),
            'unique_crashes': len(unique_hashes),
            'by_exploitability': by_exploitability,
            'by_crash_type': by_crash_type,
            'interesting_crashes': len(self.filter_interesting(crashes))
        }

    def prioritize_crashes(self, crashes: List[Dict]) -> List[Dict]:
        """Prioritize crashes by exploitability.

        Args:
            crashes: List of analyzed crashes

        Returns:
            List of crashes sorted by priority
        """
        return sorted(crashes, key=lambda c: c['priority'], reverse=True)

    def _calculate_priority(self, exploitability: str, crash_type: str) -> int:
        """Calculate priority score for crash.

        Args:
            exploitability: Exploitability rating
            crash_type: Crash type

        Returns:
            Priority score (higher = more important)
        """
        priority = 0

        # Exploitability priority
        if exploitability == 'high':
            priority += 10
        elif exploitability == 'medium':
            priority += 5
        elif exploitability == 'low':
            priority += 2

        # Crash type priority
        high_value_types = ['buffer_overflow', 'format_string', 'use_after_free']
        if crash_type in high_value_types:
            priority += 5

        return priority

    def calculate_similarity(self, crash1: bytes, crash2: bytes) -> float:
        """Calculate similarity between two crashes.

        Args:
            crash1: First crash data
            crash2: Second crash data

        Returns:
            Similarity score (0.0-1.0)
        """
        # Simple similarity based on size and content overlap
        size_diff = abs(len(crash1) - len(crash2))
        max_size = max(len(crash1), len(crash2))

        if max_size == 0:
            return 1.0

        size_similarity = 1.0 - (size_diff / max_size)

        # Check byte overlap
        min_len = min(len(crash1), len(crash2))
        if min_len > 0:
            matching_bytes = sum(1 for i in range(min_len) if crash1[i] == crash2[i])
            byte_similarity = matching_bytes / min_len
        else:
            byte_similarity = 0.0

        # Combined similarity
        return (size_similarity + byte_similarity) / 2

    def detect_mitigations(self, crash_info: Dict) -> Dict:
        """Detect exploit mitigations present.

        Args:
            crash_info: Crash information dictionary

        Returns:
            Dictionary of detected mitigations
        """
        mitigations = {
            'stack_canary': crash_info.get('stack_canary', False),
            'nx': crash_info.get('nx', True),  # Assume NX by default
            'aslr': crash_info.get('aslr', True),  # Assume ASLR by default
            'pie': crash_info.get('pie', False)
        }

        return mitigations
